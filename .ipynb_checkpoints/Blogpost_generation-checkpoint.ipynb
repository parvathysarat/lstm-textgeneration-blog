{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODfOC4Sm077z"
   },
   "source": [
    "## Loading and Generating sequences from Blog Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1552402961692,
     "user": {
      "displayName": "Parvathy Sarat",
      "photoUrl": "https://lh3.googleusercontent.com/-ciC2NfR29T8/AAAAAAAAAAI/AAAAAAAAIDE/922zI30BYXQ/s64/photo.jpg",
      "userId": "17571089196613945845"
     },
     "user_tz": -330
    },
    "id": "RJpS--M9xtzp",
    "outputId": "3e82fb67-0489-44d3-9c8d-bb78a80082e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i took out reading from my list of hobbies a couple of years ago. i remember going to public library to borrow algorithms texts last year. the year before that i browsed the shelves for books on devel\n",
      "i took out reading from my list of hobbies a couple of years ago  i remember going to public library to borrow algorithms texts last year  the year before that i browsed the shelves for books on devel\n",
      "['i', 'took', 'out', 'reading', 'from', 'my', 'list', 'of', 'hobbies', 'a', 'couple', 'of', 'years', 'ago', 'i', 'remember', 'going', 'to', 'public', 'library', 'to', 'borrow', 'algorithms', 'texts', 'last', 'year', 'the', 'year', 'before', 'that', 'i', 'browsed', 'the', 'shelves', 'for', 'books', 'on', 'development', 'studies', 'and', 'sociology', 'this', 'year', 'ive', 'visited', 'only', 'on', 'behalf', 'of', 'my', 'mother', 'to', 'get', 'her', 'yoga', 'books', 'from', 'the', 'ground', 'floor', 'main', 'section', 'it', 'is', 'quite', 'probable', 'that', 'i', 'was', 'never', 'an', 'avid', 'reader', 'i', 'mostly', 'read', 'what', 'achu', 'annan', 'suggested', 'bought', 'me', 'an', 'omnibus', 'for', 'every', 'birthday', 'when', 'i', 'was', 'young', 'a', 'lot', 'of', 'paulo', 'coelho', 'from', 'our', 'school', 'library', 'those', 'standard', 'books', 'almost', 'everyone', 'reads', 'kite', 'runner', 'to', 'kill', 'a', 'mockingbird', 'jd', 'salinger', 'he', 'bought', 'me', 'cats', 'eye', 'in', 'my', 'final', 'year', 'and', 'it', 'was', 'great', 'for', 'some', 'reason', 'i', 'really', 'liked', 'reading', 'books', 'written', 'by', 'women', 'there', 'was', 'this', 'book', 'by', 'a', 'lady', 'i', 'forget', 'the', 'name', 'but', 'im', 'sure', 'i', 'have', 'it', 'written', 'down', 'in', 'one', 'of', 'my', 'diaries', 'it', 'talked', 'of', 'suitors', 'and', 'hemming', 'lines', 'dainty', 'glass', 'cups', 'taken', 'out', 'in', 'the', 'afternoons', 'with', 'designs', 'on', 'them', 'extra', 'linen', 'stored', 'away', 'in', 'cupboards', 'and', 'wardrobes', 'recipe', 'books', 'with', 'extra', 'scribblings', 'in', 'pencil', 'on', 'margins', 'and', 'double']\n",
      "Total Tokens: 32523\n",
      "Unique Tokens: 5921\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load document\n",
    "in_filename = 'blogdata_nonextline.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "\n",
    "def clean_doc(doc):\n",
    "  #print(doc)\n",
    "  doc = doc.replace('.', ' ')\n",
    "  print(doc[:200])\n",
    "  tokens = doc.split(' ') \n",
    "  tokens = [word for word in tokens if word.isalpha()]\n",
    "  tokens = [word.lower() for word in tokens]\n",
    "  return tokens\n",
    "  \n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1552402966454,
     "user": {
      "displayName": "Parvathy Sarat",
      "photoUrl": "https://lh3.googleusercontent.com/-ciC2NfR29T8/AAAAAAAAAAI/AAAAAAAAIDE/922zI30BYXQ/s64/photo.jpg",
      "userId": "17571089196613945845"
     },
     "user_tz": -330
    },
    "id": "h3uGmSjNR0cc",
    "outputId": "bde1dbd9-88a1-4454-c153-5295a5f124ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 32502\n"
     ]
    }
   ],
   "source": [
    "length = 20 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8eIuGnuLa7Dy"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VeuRQo22VQcZ"
   },
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aNnqhv7SVRY7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# save sequences to file\n",
    "\n",
    "out_filename = 'blogdata_nonl_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1001,
     "status": "ok",
     "timestamp": 1552402972291,
     "user": {
      "displayName": "Parvathy Sarat",
      "photoUrl": "https://lh3.googleusercontent.com/-ciC2NfR29T8/AAAAAAAAAAI/AAAAAAAAIDE/922zI30BYXQ/s64/photo.jpg",
      "userId": "17571089196613945845"
     },
     "user_tz": -330
    },
    "id": "4jUFvSq-VXC0",
    "outputId": "0f6fbce8-b230-47ef-d908-e865c11cce12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i took out reading from my list of hobbies a couple of years ago i remember going to public library to', 'took out reading from my list of hobbies a couple of years ago i remember going to public library to borrow', 'out reading from my list of hobbies a couple of years ago i remember going to public library to borrow algorithms', 'reading from my list of hobbies a couple of years ago i remember going to public library to borrow algorithms texts', 'from my list of hobbies a couple of years ago i remember going to public library to borrow algorithms texts last', 'my list of hobbies a couple of years ago i remember going to public library to borrow algorithms texts last year', 'list of hobbies a couple of years ago i remember going to public library to borrow algorithms texts last year the', 'of hobbies a couple of years ago i remember going to public library to borrow algorithms texts last year the year', 'hobbies a couple of years ago i remember going to public library to borrow algorithms texts last year the year before', 'a couple of years ago i remember going to public library to borrow algorithms texts last year the year before that']\n"
     ]
    }
   ],
   "source": [
    "print(sequences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vS7ItorSVd1b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyQXLGR-Vu4o"
   },
   "source": [
    "##**LANGUAGE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gUQ9L1xQVxh9"
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# load\n",
    "in_filename = 'blogdata_nonl_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2913511,
     "status": "ok",
     "timestamp": 1552372716504,
     "user": {
      "displayName": "Parvathy Sarat",
      "photoUrl": "https://lh3.googleusercontent.com/-ciC2NfR29T8/AAAAAAAAAAI/AAAAAAAAIDE/922zI30BYXQ/s64/photo.jpg",
      "userId": "17571089196613945845"
     },
     "user_tz": -330
    },
    "id": "1Vr6RyzxWI8J",
    "outputId": "ebf1b457-1eeb-420f-e159-d043a33efc61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 50)            296100    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5922)              598122    \n",
      "=================================================================\n",
      "Total params: 1,045,122\n",
      "Trainable params: 1,045,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      " 1024/32502 [..............................] - ETA: 1:52 - loss: 8.6830 - acc: 0.0410"
     ]
    }
   ],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-JK0_I_aWO8r"
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "\n",
    "\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oz0U0V0qYxHZ"
   },
   "source": [
    "### Load Language Model to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2278,
     "status": "error",
     "timestamp": 1552402814831,
     "user": {
      "displayName": "Parvathy Sarat",
      "photoUrl": "https://lh3.googleusercontent.com/-ciC2NfR29T8/AAAAAAAAAAI/AAAAAAAAIDE/922zI30BYXQ/s64/photo.jpg",
      "userId": "17571089196613945845"
     },
     "user_tz": -330
    },
    "id": "hnRoeu8mY2bM",
    "outputId": "90fafedb-04a3-4590-a587-4e367ecaf4ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1ce9a0d747e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# load cleaned text sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0min_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'blogdata_nonl_sequences.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1ce9a0d747e9>\u001b[0m in \u001b[0;36mload_doc\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# open the file as read only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# read all text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'blogdata_nonl_sequences.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'blogdata_nonl_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(\"SEED:\"+seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 40)\n",
    "print(\"GENERATED:\"+generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4B_jshzWGiYE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Blogpost_generation.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "nithin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
